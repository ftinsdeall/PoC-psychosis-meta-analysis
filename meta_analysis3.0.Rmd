---
title: "meta_analysis3.0"
author: "Francesca Tinsdeall"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE, }
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, 
               knitr,
               here,
               DT,
               readxl, 
               metafor,
               clubSandwich,
               orchaRd, 
               MuMIn,
               patchwork,
               GoodmanKruskal,
               networkD3,
               ggplot2,
               plotly,
               ggsignif,
               visdat,
               ggalluvial,
               ggthemr, 
               cowplot,
               grDevices,
               png,
               grid,
               gridGraphics,
               pander,
               formatR,
               rmdformats,
               pander,
               metapower, 
               cowplot, 
               gt)

library(metameta)

SMD.g <- read.csv('meta.csv')
write.csv(SMD.g, 'meta.csv')
```


# Single level random effects model 

## 1. Calculate effect size (SMD) - already done on previous meta-analysis versions

Hedges g to account for small sample sizes (default for SMD when using the escalc() function) - Hedge’s g (statistically corrects for variance that may be introduced when sample sizes are small (Larry V. Hedges 1981)). Of note, I use the “true” number of control animals (c.n), where the sample size of the control group is divided by the number of treatment groups it serves (if one control group serves multiple treatment groups), to avoid control groups contributing multiple times to the calculation of an effect size (Vesterinen et al., 2014). 


## 2. Fit a simple random effects model - assumes independence 

```{r - random effects model, results='hide'}
random_SMD <- metafor::rma(yi = SMD, 
                               vi = SMDV, 
                               test = "t", # the t-distribution is specified 
                                           #to calculate confidence intervals
                               data = SMD.g, 
                               method = 'REML'
                              ) 
summary(random_SMD)

```
Pooled effect size estimate = -0.3983 
Significance level of effect size estimate = 0.001
Standard error of estimate = 0.0401
I2 value = 62.82% 
Interpret I2 of 25%, 50%, and 75% as small, medium, and large variance, respectively https://pubmed.ncbi.nlm.nih.gov/12111919/


# Determine random effect variables that should be included in model - have to use ML rather than REML so that anova comparison works 

## 1. Introduce study_id as a random variable (between study variation)

Study identity (study_id_str) - unique ID for each extracted original experimental paper; modelling it as a random-effect means to allow true effect sizes to vary across studies, such that the model can estimate between-study level variance and partition between-study level heterogeneity

```{r - introduce study_id_str as a random effect, results='hide'}
ul <- rma.mv(yi = SMD, V = SMDV, data = SMD.g, method = 'ML') #unilevel for comparison

ml_study <- rma.mv(yi = SMD, V = SMDV, random = ~ 1 | study_id_str, data = SMD.g, method = 'ML') #2-level with study_id_str

summary(ml_study)
i2_ml(ml_study)
```
Pooled effect size = -0.5687
Significance level of effect size estimate = 0.0001
Standard error of estimate = 0.1026
I2 value = 75.9% 

```{r - test whether introducing study_id_str as a random effect improves model fit, results='hide'}
anova.rma(ml_study, ul)
```
Introducing study_id_str as a random effect significantly improves model fit (LRT = 201.7, p < 0.0001)
AIC for ml_study = 441.39
AIC for ul = 641.1

## 2. Introduce effect_ID as a random variable (within study variation)

Effect size identity (effect_id) - unique ID for each pairwise comparison used to calculate effect sizes; modelling it as a random-effect means to allow true effect sizes to vary within studies, such that the model can estimate within-study (effect size) level variance and partition within-study (effect size) level heterogeneity. 


```{r - introduce comparison_id as a random effect, results='hide'}
ml_effect <- rma.mv(yi = SMD, V = SMDV, random = ~ 1 | effect_id, data = SMD.g, method = 'ML')

summary(ml_effect)
i2_ml(ml_effect)
```
Pooled effect size = -0.3981
Significance level of effect size estimate = <0.0001
Standard error of estimate = 0.0400

I2 value = 62.6% 

```{r - test whether introducing comparison_id as a random effect improves model fit, results='hide'}
anova.rma(ml_effect, ul)
```
Introducing effect_id as a random effect significantly improves model fit (LRT = 113.1, p < 0.0001
AIC for ml_effect = 530
AIC for ul = 641.1

## 3. Combine study_id and effect_id to generate 3-level RMA model

```{r - introduce both study_id and comparison_id as random effects, results='hide'}
ml_study.effect <- rma.mv(yi = SMD, V = SMDV, random = ~1 | study_id_str / effect_id, data = SMD.g, method = 'ML')

summary(ml_study.effect)
i2_ml(ml_study.effect)
```
Pooled effect size = -0.5671
Significance level of effect size estimate = <0.0001
Standard error of estimate = 0.1022

Overall I2 value = 76.8% 
I2 value attributable to between study variance = 70.6%
I2 value attributable to within study variance = 6.18%

**random = ~1 | study_id_str / effect_id** tells r that the effect_id variable is a random effect within the study_id_str variable

## 4. Assess whether both study_id and comparison_id are needed as random effects in the RMA model 
```{r - test whether introducing comparison_id as a random effect to ml_study improves model fit, results='hide'}
anova.rma(ml_study.effect, ml_study)
```
Introducing effect_id as a random effect (in addition to study_id_str) significantly improves the fit of the model (LRT = 6.3919, p = 0.0115)
AIC for ml_study.effect = 437
AIC for ml_study = 441.39

Therefore, at least a 3-level random effect model is needed to account for non-independence in my data. 
Level 1: the sampling variance effect (used to account for sampling/measurement error effect in effect size)
Level 2: the effect_id (used to account for within-study random effect and uses corresponding variance component (sigma within) to capture within study-specific heterogeneity) 
Level 3: the study_id (used to account for between-study random effect and uses corresponding variance component (sigma between) to capture study-specific heterogeneity)

This also corroborates previous findings demonstrating that animal studies benefit from at least a 3-level random effects model due to the level of non-independence between effect sizes commonly found within them. 


# Other levels of non-independence in my data at the between study level (i.e. account for additional between study non-independence)

## Effect sizes from the same comparison cohorts 
All effect sizes generated from the same comparison cohorts (I define a comparison cohort as one polyI:C cohort and its corresponding control cohort) will exhibit non-independence. The cohort variable groups effect sizes from the same comparison cohorts, so introducing 'cohort' as a random effect term should account for this level of non-independence 

```{r - introduce cohort as a random effect, results='hide'}
ml_study.cohort.effect <- rma.mv(yi = SMD, V = SMDV, random = ~1 | study_id_str / cohort / effect_id, data = SMD.g, method = 'ML')
summary(ml_study.cohort.effect)
i2_ml(ml_study.cohort.effect)
```
Pooled effect size estimate = -0.5619
Significance level of effect size estimate = <0.0001
Standard error of estimate = 0.1010

Overall I2 value = 76.4% 

I2 value attributable to variance between studies = 61.11% (i.e. 61.1% of heterogeneity not attributed to sampling error is explained by between study variance)
I2 value attributable to within study variance between cohorts = 13.45%
I2 value attributable to residual within study variance = 1.83% 

so...
I2 value attributed to between study variance = 61.11% 
I2 value attributed to within study variance not due to sampling error = 15.28% 

```{r - test the fit of model now htat includes cohort as fixed effect, results='hide'}
anova.rma(ml_study.cohort.effect, ml_study.effect)
```
AIC for ml_study.effect.cohort = 427.6199
AIC for ml_study.effect = 437.0005
LRT = 11.2806, p = 0.0007 
Including cohort as a fixed effect significantly improves the fit of the model 

# Levels of non-independence accounted for by the inclusion of relevant moderator variables within my model 

## Effect sizes from the same species will be correlated with each other - introducing species as a moderator variable accounts for this 

## Effect sizes from the same strain will be dependent on each other - introducing strain as a moderator variable accounts for this 
Strain and species need to be included in the model as an interaction moderator term as the correlation observed within strains is also correlation observed within a given species 

## Effect sizes from the same outcome testing protocol conducted at the same time will exhibit non-independence - this is accounted for by including time as a moderator variable 
All effect sizes generated from a given PPI testing protocol within a study at the same time may exhibit non-independence. For example, the room temperature may have been higher at the time of PPI testing on PND30 compared to PND70. Similarly, PPI testing on PND30 may have been carried out in the morning, whilst it was carried out in the afternoon on PND70 (factors such as diurnal cortisol secretion patterns may affect PPI). 

## Effect sizes from the same outcome testing protocol may exhibit non-independence - this is currently not accounted for by my model. I don't yet have a moderator variable for pre-pulse as 29 effect sizes came from %PPI values averaged across multiple prepulse intensities, so I held off introducing prepulse intensity as a moderator as I didn't want to reduce the no. of datapoints available to fit the model. It may be more feasible to introduce pulse as moderator variable as only 1 effect size came from a %PPI value averaged across pulse intensity (so would only need to exclude one effect size)
All effect sizes generated from a given PPI testing protocol within a study may be correlated. For instance, one investigator may have performed testing for 65dB prepulses and another performed testing for 70dB prepulses within a study, and individual investigators may have recorded PPI results slightly differently. 

# Change method back to REML for improved model and assess output 
```{r - change method back to REML, results='hide'}
ml_study.cohort.effect.re <- rma.mv(yi = SMD, V = SMDV, random = ~1 | study_id_str / cohort / effect_id, data = SMD.g, method = 'REML')

summary(ml_study.cohort.effect.re)
i2_ml(ml_study.cohort.effect.re)
```
Pooled effect size = -0.5635
Significance level of effect size estimate = <0.0001
Standard error of estimate = 0.1024

Overall I2 value = 76.9% 

I2 value attributable to variance between studies = 62.0% 
I2 value attributable to within study variance between cohorts = 13.09%
I2 value attributable to residual within study variance = 1.79% 

I2 value attributed to within study variance not due to sampling error = 14.9% 

# Construct VCV matrix to account for non-independence in sampling errors (sampling covariance)

Multiple effect sizes from the same study can result in two types of data non-independence: independence between effect size estimates (i.e., correlated effect size) and non-independence between sampling errors (i.e., correlated errors). The use of multilevel model with an appropriate random-effects structure can only capture the non-independence between effect size estimates. Using a variance-covariance (VCV) matrix within a multilevel model can capture the non-independence between sampling errors. Given that many of the studies within my meta-analysis use the same control cohort for multiple disease cohorts, there will be correlation of sampling errors within my data. 

## 1. Construct VCV matrix 

Although we are not able to construct an exact VCV  matrix (due to missing rho), we can impute a **VCV** matrix by assuming a constant sampling correlation $\rho$ across different studies. Assuming a rho value of 0.5, seems to be a plausible and safe assumption across many situations

```{r - impute a VCV matrix assuming rho value of 0.5, results='hide'}
VCV.5 <- vcalc(vi = SMDV, 
             cluster = study_id_str, 
             obs = effect_id, 
             data = SMD.g, 
             rho = 0.5)
```

## 2. Fit RMA with VCV (assuming .5 rho) instead of SMDV

```{r - fit multilevel model with values from VCV.5 matrix as effect size variance, results='hide'}
ml_study.cohort.effect.re.VCV.5 <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, data = SMD.g, method = 'REML')
summary(ml_study.cohort.effect.re.VCV.5)
i2_ml(ml_study.cohort.effect.re.VCV.5)
```
Pooled effect size = -0.5001
Significance level of effect size estimate = <0.0001
Standard error of estimate = 0.0903

Overall I2 value = 71.7% 

I2 value attributable to variance between studies = 35.2% 
I2 value attributable to within study variance between cohorts = 18.02%
I2 value attributable to residual within study variance = 18.49% 

I2 value attributed to within study variance not due to sampling error = 36.5% 

## 3. Orchard plot of RMA with VCV 

```{r - orchard plot of RMA with VCV}
orchard_plot(ml_study.cohort.effect.re.VCV.5, xlab = "Standardised mean difference (SMD)", group = "study_id_str", data = SMD.g, k = TRUE, g = TRUE, transfm = "none", angle = 0)
```

Compared to when not accounting for sampling error covariance...

```{r - orchard plot of RMA without VCV}
orchard_plot(ml_study.cohort.effect.re, xlab = "Standardised mean difference (SMD)", group = "study_id_str", data = SMD.g, k = TRUE, g = TRUE, transfm = "none", angle = 0)
```


## 4. Sensitivity analysis 
Heterogeneity estimates in particular seem to be sensitive to the value of assumed rho used to impute VCV matrix 

### Rho 0.2
```{r - impute a VCV matrix assuming rho value of 0.2, results='hide'}
VCV.2 <- vcalc(vi = SMDV, 
             cluster = study_id_str, 
             obs = effect_id, 
             data = SMD.g, 
             rho = 0.2)
```

```{r fit multilevel model with values from VCV.2 matrix as effect size variance, results='hide'}
ml_study.cohort.effect.re.VCV.2 <- rma.mv(yi = SMD, V = VCV.2, random = ~1 | study_id_str / cohort / effect_id, data = SMD.g, method = 'REML')
summary(ml_study.cohort.effect.re.VCV.2)
i2_ml(ml_study.cohort.effect.re.VCV.2)
```
Pooled effect size = -0.5403
Significance level of effect size estimate = <0.0001
Standard error of estimate = 0.0979

Overall I2 value = 74.9% 

I2 value attributable to variance between studies = 53.22% 
I2 value attributable to within study variance between cohorts = 14.93%
I2 value attributable to residual within study variance = 6.78% 

I2 value attributed to within study variance not due to sampling error = 21.68% 

### Rho 0.7
```{r - impute a VCV matrix assuming rho value of 0.7, results='hide'}
VCV.7 <- vcalc(vi = SMDV, 
             cluster = study_id_str, 
             obs = effect_id, 
             data = SMD.g, 
             rho = 0.7)
```

```{r fit multilevel model with values from VCV.7 matrix as effect size variance, results='hide'}
ml_study.cohort.effect.re.VCV.7 <- rma.mv(yi = SMD, V = VCV.7, random = ~1 | study_id_str / cohort / effect_id, data = SMD.g, method = 'REML')
summary(ml_study.cohort.effect.re.VCV.7)
i2_ml(ml_study.cohort.effect.re.VCV.7)
```
Pooled effect size = -0.4688
Significance level of effect size estimate = <0.0001
Standard error of estimate = 0.0844

Overall I2 value = 69.52% 

I2 value attributable to variance between studies = 19.53% 
I2 value attributable to within study variance between cohorts = 20.35%
I2 value attributable to residual within study variance = 29.64% 

I2 value attributed to within study variance not due to sampling error = 49.99% 

# Meta-analysis of variation (lnVR and lnCVR)

The log-transformed variability ratio, lnVR, quantifies the difference in variance (SD) around the mean between two arms (i.e., estimating inter-individual variability between two arms or heterogeneity of treatment effect; Nakagawa et al., 2015; Senior et al., 2020). The log-transformed coefficient of variation ratio (lnCVR) is a mean-adjusted version of lnVR, where the indirect impact of mean on its variability is controlled for (i.e., accounting for the mean-variance relationship; Nakagawa et al., 2015; Volkmann et al., 2020). 

I wanted to caluclate lnVR and lnCVR to investigate the external validity of the polyI:C MIA developmental model of psychosis. You would hope that there would not be a significant difference in inter-individual variability between the polyI:C and control cohort groups. If the lnVR or lnCVR value is positive, this indicates that polyI:C exposure during gestation increases the inter-individual variability in offspring pre-pulse inhibition response. If this is the case, this suggests that reproducibility of %PPI results derived from the polyI:C model of psychosis is limited - thus identifying a shortcoming in the external validity of the model. 

## 1. Calculate lnVR 

```{r - lnVR, results='hide'}
lnVR <- escalc("VR",                          
               m1i = d.mean.adj,        
               n1i = d.n, 
               sd1i = d.sd, 
               m2i = c.mean.adj, 
               n2i = c.n, 
               sd2i = c.sd,
               data = SMD.g, 
               digits = 3,
               append = TRUE) %>% 
  rename(VR = yi, 
         VRV = vi)
```

## 1.2 Fit 4 level RMA model to lnVR

```{r - fit 4 level model to lnVR, results='hide'}
ml_study.cohort.effect.re_VR <- rma.mv(yi = VR, V = VRV, random = ~1 | study_id_str / cohort / effect_id, data = lnVR, method = 'REML')
summary(ml_study.cohort.effect.re_VR)
```
Pooled lnVR estimate = 0.0527
p = 0.2596
The results of this analysis suggest that polyI:C exposure during gestation slightly increases the inter-individual variation in the pre-pulse inhibition response of offspring, but this is not even close to significance. Thus, the results of meta-analysis using lnVR as an effect size suggest that there is no significant different in inter-individual variance of mean %PPI between polyI:C treated and vehicle treated cohorts

## 2. Calculate lnCVR - mean-adjusted version of lnVR

```{r - lnCVR, results='hide'}
lnCVR <- escalc("CVR",                          
               m1i = d.mean.adj,        
               n1i = d.n, 
               sd1i = d.sd, 
               m2i = c.mean.adj, 
               n2i = c.n, 
               sd2i = c.sd,
               data = SMD.g, 
               digits = 3,
               append = TRUE) %>% 
  rename(CVR = yi, 
         CVRV = vi)
```

## 2.1 Fit 5 level RMA model to lnCVR

```{r - fit multilevel RMA to lnCVR, results='hide'}
ml_study.cohort.effect.re_CVR <- rma.mv(yi = CVR, V = CVRV, random = ~1 | study_id_str / cohort / effect_id, data = lnCVR, method = 'REML')
summary(ml_study.cohort.effect.re_CVR)

```
Pooled lnCVR estimate = -0.0914
p = 0.0548
The results of meta-analysis using lnCVR as an effect size confirm that that there is no significant different in inter-individual variance of mean %PPI between polyI:C treated and vehicle treated cohorts, even after controlling for mean-variance relationship. However, the p value is much closer to the 0.05 significance threshold - a larger meta-analysis may find that polyI:C treatment does significantly increase inter-individual variability in the offspring of dams. Nonetheless, this analysis suggests that the polyI:C MIA model of psychosis has good reproducibility, a factor that contributes to its external validity 

https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001009 (paper discussing results reproducibility in preclinical models)

## 2.2 Orchard plot of lnCVR 

```{r - orchard plot of lnCVR}
orchard_plot(ml_study.cohort.effect.re_CVR, xlab = "lnCVR", group = "study_id_str", data = lnCVR, k = TRUE, g = TRUE, transfm = "none", angle = 0)
```


# Power analysis to calculate the power of individual studies included within my meta-analysis (sensitivity analysis)

Using https://journals.sagepub.com/doi/10.1177/25152459221147260 

Package is designed to work with non-independent effect sizes (i.e. one effect size per study), however I got in touch with the author of the package and one of the workarounds he suggested was performing a power analysis using the effect size with the highest variance from each study, and then repeating using the effect size with the lowest variance, and then checking that there the values were similar for both conditions (he predicted that the power would be relatively stable)

## Power from highest variance effect size for each study

```{r - create dataset for use with metameta: select effect size with highest variance for each study (a priori decision), results='hide'} 
SMD.g.highest_var <- SMD.g %>% 
  group_by(study_id_str) %>% 
  top_n(1, SMDV) %>% 
  select(study_id_str, SMD, SMDV) %>% 
  mutate(sei = sqrt(SMDV), 
         yi = SMD) %>% 
  select(-SMDV) 

power_h <- mapower_se(SMD.g.highest_var, observed_es = -.5001, name = 'psychosis MA')
power_h_res <- power_h$dat

```
This analysis suggests that none of the studies included within this meta-analysis had the power to detect the effect sizes they reported with 80% confidence 

## Power from lowest variance effect size for each study 

```{r - create dataset for use with metameta: study_id_str, SMD and SD: select effect size with lowest variance for each study (a priori decision), results='hide'} 
SMD.g.lowest_var <- SMD.g %>% 
  group_by(study_id_str) %>% 
  top_n(-1, SMDV) %>%  
  select(study_id_str, SMD, SMDV) %>% 
  mutate(sei = sqrt(SMDV), 
         yi = SMD) %>% 
  select(-SMDV) 

power_l <- mapower_se(SMD.g.lowest_var, observed_es = -.5001, name = 'psychosis MA')
power_l_res <- power_l$dat
```
This analysis also suggests that none of the studies included within this meta-analysis had the power to detect the effect sizes they reported with 80% confidence (very little difference in power estimates)

### Presenting the results from highest and lowest variance effect sizes from each study side by side (sensitivity analysis results)
*unsure as to how you present the results of a sensitivity analysis properly*

```{r - dataset with power to detect effect sizes with lowest and highest variances for each study (sensitivity analysis), results='hide'}
power_res <- power_h_res %>%
  select(study_id_str, power_es_observed) %>% 
  rename(power_highest_var = power_es_observed) %>% 
  left_join(power_l_res) %>% 
  select(study_id_str, power_highest_var, power_es_observed) %>% 
  rename(power_lowest_var = power_es_observed) %>% 
  mutate(discrepancy = power_lowest_var - power_highest_var) %>% 
  select(-study_id_str)
```

```{r - print table}
pander(power_res, graph.fontsize = 7)
```
Each line represents one study. Studies with 0 discrepancies are studies that only reported 1 effect size. The sensitivity analysis shows that power estimates for individual studies remain relatively constant for both conditions 

# Power analysis for my 4-level model (no moderators)
The metapower package requires the average size of cohorts contributing to each effect size (this is assumed to be one per study). Given that many of the studies included within my meta-analysis have multiple effect sizes, I have made the a priori decision to calculate power assuming one effect size per study (to remove effect of non-independence - i.e. assume that all effect sizes within a study are dependent) and calculate the average of the smallest cohort sizes (contributing to each effect size) for each study i.e. be conservative/assume the worst to use as my sample size within study

```{r - calculate the average (conservative) sample size for studies, results='hide'}
ss <- SMD.g %>% 
  select(study_id_str, disease_cohort_str, control_cohort_str, d.n, c.n) %>% 
  distinct() 

disease_ss <- ss %>% 
  select(study_id_str, disease_cohort_str, d.n) %>% 
  distinct() %>% 
  group_by(study_id_str) %>% 
  summarise(total_disease = sum(d.n)) 

control_ss <- ss %>% 
  select(study_id_str, control_cohort_str, c.n) %>% 
  distinct() %>% 
  group_by(study_id_str) %>% summarise(total_control = sum(c.n))

disease_ss_con <- ss %>% select(study_id_str, d.n) %>% group_by(study_id_str) %>% top_n(-1, d.n)
mean(disease_ss_con$d.n) #14.54
control_ss_con <- ss %>% select(study_id_str, c.n) %>% group_by(study_id_str) %>% top_n(-1, c.n)
mean(control_ss_con$c.n) #13.51
#mean conservative size of cohorts contributing to each effect size = 14.0
```
Average size of cohorts contributing to each effect size if being conservative = 14

```{r - power analysis for whole meta-analysis, results = 'hide'}
power <- mpower(effect_size = 0.5001,
                 study_size = 14,
                 k = 48 ,
                 i2 = 0.717,
                 es_type = "d")
summary(power)
```
My meta-analysis is adequately powered to detect the estimated pooled effect size and for that level of heterogeneity (92.2% power). This meta-analysis is therefore an example of how although individual included studies may not have sufficient statistical power to reliably detect a wide range of effect sizes, the synthesis of several of these studies into a summary effect size can increase statistical power, and achieve that sufficient to reliably detect the presence of a significant effect. 

```{r - plot of power analysis for whole meta-analysis}
power_plot <- plot_mpower(power)
power_plot <- power_plot + ggtitle("Power analysis for a meta-analysis of 48 studies with a mean total sample size of 14 (per effect size estimate) \nto detect pooled effect size of 0.5602 assuming 76.8% heterogeneity")
power_plot
```

# Test for publication bias 

Identifying publication bias is a crucial and mandatory procedure of a meta-analysis because the validity of meta-analytic evidence would be undermined if publication bias occurs (Augusteijn et al., 2019*, Nakagawa et al., 2017, Van Aert et al., 2019b). 
Common methods for testing publication bias are invalid if effect sizes are statistically dependent. Therefore, funnel plots, conventional Egger’s regression and trim-and-fill tests are often not suitable to test publication bias for animal data meta-analyses (Rodgers and Pustejovsky, 2021*, Sterne et al., 2001a).

One of the limitations of funnel plots is that funnel asymmetry can be caused not just by publication bias (as in Figure 3b, missing large effect sizes of high uncertainties or unexpected missing data points can create such asymmetry; see also Terrin et al., 2005). For instance, heterogeneity among effect sizes can create asymmetries of many kinds. Given the high degree of heterogeneity in my data, a standard funnel plot is unlikely to be informative about publication bias. 

To account for some of the heterogeneity, several researchers recommend plotting residuals from a meta-regression model (e.g. Roberts & Stanley, 2005) *Nakagawa 2022* 

The heterogeneity in my data-set however is unexplained, and even when biological moderators and random effects variables are combined, this only explain 83.4% of the variance/heterogeneity. As a result, the remaining heterogeneity after plotting residuals might still generate asymmetry in a residual funnel plot. Furthermore, the method of plotting residuals from a meta-regression model onto a funnel plot to account for some of the heterogeneity relies on the assumption that all the residuals are independent. Given that there is correlation of sampling (error) effects within my data, this assumption would be violated, thus representing another reason why this method cannot be used. 


## 1. Extended Egger's regression (small study effect)
The most common testable form of publication bias is the small-study effect where small studies (i.e., small sample size) often tend to report large effect sizes (Sterne et al., 2000). A multilevel version of Egger’s regression has recently been proposed to overcome the limitations of conventional Egger's regression and allow assessment of small study effect in meta-analyses containing non-independent effect sizes (Fernández-Castilla et al., 2021, Nakagawa et al., 2021a). In brief, sampling error is added as a moderator variable to a multilevel random effects model. 

A potential improvement on this approach is to use ‘effective sample size’ instead of sampling error (Nakagawa et al., 2021a** - https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13724). When using SMD as an effect size metric, using Egger’s test to identify small-study effect may produce false-positive results. The SE of a given SMD is artifactually correlated with the SMD, meaning that SMD's SE is dependent on SM. 

```{r - Eggers regression using correct SMD sampling error, results='hide'}
SMD.g$SMDSE_c <- with(SMD.g ,sqrt((d.n + c.n)/(d.n*c.n)))

pub_bias.ss <- rma.mv(yi = SMD, V = SMDV, random = ~1 | study_id_str / cohort / effect_id, 
                   mods = ~ SMDSE_c, 
                   data = SMD.g, 
                   test = 't',
                   method = 'REML')

summary(pub_bias.ss)
```
Regression slope of the adapted SE = -1.309
p = 0.1636
Slope is is not statistically different from zero. This means that smaller studies (with larger sampling error) do not have larger effect sizes i.e. no small-study effect exists in this dataset

```{r - visualise Eggers regression, results='hide'}
bubble_plot(pub_bias.ss, mod = "SMDSE_c", 
            xlab = "Corrected sampling error", ylab = "Effect size estimates (SMD)",
            group = "study_id_str",
            data = SMD.g, legend.pos = "none") 
```

## 2. Time-lag bias 
Similarly, including publication year as a moderator variable can be used to detect the decline effect i.e., time-lag bias (defined as the temporal instability of the magnitude of effect sizes), the implication of which is underappreciated (Grainger et al., 2020, Koricheva and Kulinskaya, 2019

```{r - time-lag bias, results='hide'}
pub_bias.tl <- rma.mv(yi = SMD, V = SMDV, random = ~1 | study_id_str / cohort / effect_id, 
                   mods = ~ year, 
                   data = SMD.g, 
                   test = 't',
                   method = 'REML')

summary(pub_bias.tl)
```
Regression slope of year of publication = 0.0290
p = 0.2524
The regression slope is not statistically different from zero. This indicates that studies with statistically significant findings do not tend to be published earlier than those with "negative" results, i.e. no time-lag bias exists in this dataset

```{r - visualise relationship between year of publication and SMD (time-lag bias) in dataset}
bubble_plot(pub_bias.tl, mod = "year", 
            xlab = "Year of publication", ylab = "Effect size estimates (SMD)",
            group = "study_id_str",
            data = SMD.g, legend.pos = "none") 
```

## 3. Sampling one effect size per study to assess publication bias using TAF and step-selection (whilst accounting for dependence)
Using code from https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13724

Many of the standard methods of testing for publication bias (e.g. trim-and-fill, step function selection models) are still useful in the presence of non-independent data if effect sizes are either aggregated for each study, or one effect size is randomly sampled from each study (Nakagawa 2022)

Here I use functions written by Nakagawa et al. (2022) that perform (i) random selection of one effect size per study to generate a meta-analytic dataset; (ii) fitting the publication bias test of interest; and (iii) extracting estimates from the publication bias test output. Each function is repeated 1000 times to generate 1000 samplings 

Overall means will generally not be biased using aggregated or single sample/study effect sizes (Song et al., 2020). 

### 3.1 Trim and fill analysis for publication bias using sampling

Using R0 estimator 

```{r - apply trim fill to 1000 random samples}
# function for randomly selecting 1 effect size from each study
func_S5.1.2.1 <- function(sim = 1){
  
  # splitting dataframe into each study
  study_list <- split(SMD.g, SMD.g$study_id_str)
  
  # randomly extracting one effect size per study
  SingleStudyES_SMD.g <- dplyr::bind_rows(lapply(study_list, function(x) 
    x[sample(1:nrow(x), 1),c("study_id_str","SMD","SMDV")]))
  
  # running the model on the dataframe now each study only has a single effect size
  model.tmp.r <- rma(SMD, SMDV, method="ML", test="knha",data=SingleStudyES_SMD.g)
  
  # applying trim-and-fill method
  TAF <- trimfill(model.tmp.r, estimator = 'R0')
 
  # creating a dataframe with results from the trim-and-fill methods, with:
  #n = number of missing studies
  #beta = adjusted overall effect
  df <- data.frame(sim,
                   n=TAF$k0,
                   beta=TAF$beta[[1]])
  
  return(df)
}

trimfill.sampling.r <- dplyr::bind_rows(lapply(1:1000, function(x) func_S5.1.2.1(x)))
```

```{r - plots of trim fill analysis}
# plotting the distribution of the number of missing studies according to the trim-and-fill method
n.r <- ggplot(data=trimfill.sampling.r,aes(x=n)) +
  geom_density(alpha=.5,fill="darkorchid4") +
  geom_vline(data=trimfill.sampling.r, aes(xintercept=median(n)),
             linetype="dashed", size=1) +
  labs(title = 'Estimated number of missing studies according to the trim-and-fill method', 
       subtitle = 'Median estimated number represented by dashed line', 
       x = 'Estimated number of missing studies', 
       y = 'Probability density')

# plotting the distribution of the adjusted overall effect according to the trim-and-fill method
betas.r <- ggplot(data=trimfill.sampling.r,aes(x=beta)) +
  geom_density(alpha=.5,fill="deeppink2") +
  geom_vline(data=trimfill.sampling.r, aes(xintercept=median(beta)),
             linetype="dashed", size=1) +
  labs(title = 'Adjusted overall effect sizes according to the trim-and-fill method', 
       subtitle = 'Median overall adjusted effect size represented by dashed line', 
       x = 'SMD', 
       y = 'Probability density')

ggarrange(n.r, betas.r, 
          labels = c("A", "B"),
          ncol = 2, nrow = 1)
```

The median estimated number of missing studies estimated by the trim-and-fill method was 5 (range = 1-18). Notably, the pooled effect size after adding in the missing studies was -0.36612, which is smaller than the effect size estimated by the multilevel meta-regression model (-0.5001). This suggests that publication bias 

### 3.2 Step-selection model 
Selection model-based methods represent the most sophisticated, complex class of publication bias methods (reviewed in Marks-Anglin & Chen, 2020a; Rothstein et al., 2005; Vevea et al., 2019). There are probably as many selection models as all other methods combined (Marks-Anglin & Chen, 2020a), but a property common to all of them is that they model how effect sizes are missing (or selected to be published), based on one or more statistical parameters, for example, p values, effect sizes or sampling variance (e.g. Carter et al., 2019; Preston et al., 2004; Rodgers & Pustejovsky, 2021; Figure 5b,c). Importantly, selection models can tolerate and model heterogeneity.

```{r - 3PM selection model based on sampling effect sizes from studies, results='hide'}
func_S5.1.2.2 <- function(sim = 1){
  
  # splitting dataframe into each study
  study_list <- split(SMD.g, SMD.g$study_id_str)
  
  # randomly extracting one effect size per study
  SingleStudyES_SMD.g <- dplyr::bind_rows(lapply(study_list, function(x) 
    x[sample(1:nrow(x), 1),c("study_id_str","SMD","SMDV")]))
  
  # running the model on the dataframe now each study only has a single effect size
  model.tmp.r.agg0 <- rma(SMD, SMDV, method="ML", test="knha",data=SingleStudyES_SMD.g)
  
  #extracting the adjusted overall effect according to the selection model for each database
  threePSM.vector.r <- selmodel(model.tmp.r.agg0, type="stepfun", steps=c(0.05))$beta[[1]]
  
  #creating a dataframe with the n.randomization.r of estimated number of missing studies and adjusted overall effects
  df <- data.frame(sim, beta=threePSM.vector.r)

  return(df)
}

# applying the function 1000 times
threePSM.sampling.r <- dplyr::bind_rows(lapply(1:1000, function(x) func_S5.1.2.2(x)))
summary(threePSM.sampling.r)
```
The median adjusted overall effect according to the 3 parameter selection model (based on the cut point of alpha=0.05) is -0.4635 (range = -0.7161, -0.03844). Whilst still smaller, this is closer to the effect size estimated by the multilevel meta-regression model than the estimate from the trim-fill method. Given that the 3-PM model can tolerate heterogeneity, this is likely the most informative test for publication bias. It demonstrates there is unlikely to be significant publication bias within this dataset. 

```{r - plotting the distribution of adjusted overall effect sizes according to the 3 parameter selection model}
ggplot(data=threePSM.sampling.r,aes(x=beta)) +
  geom_density(alpha=.5,fill="skyblue") +
  geom_vline(data=threePSM.sampling.r, aes(xintercept=median(beta)),
             linetype="dashed", size=0.5) +
  labs(title = 'Distribution of 1000 adjusted overall effect sizes according to the 3 parameter selection model', 
       subtitle = 'Median value represented by dashed line', 
       x = 'SMD', 
       y = 'Probability density')
```

## Notes on interpreting publication bias tests with caution (from Nakagawa et al., 2021a)

Few simulation studies have explicitly investigated the performance of publication bias tests with non-independent data. Two simulation studies that we are aware of supported similar models to the multilevel-regression method we proposed here (Fernandez-Castilla et al., 2021; Rodgers & Pustejovsky, 2021). In addition, a general point to take from these two simulation studies is that most methods are prone to Type 2 error, even when the methods have nominal Type 1 error rates. Therefore, not detecting publication bias in a publication bias test should not be taken as a proof of no publication bias, including for multilevel regression.

Although using averaging or sampling are not a universal solution, they may be useful in supplementing our multilevel meta-regression method. This is because all publication bias tests should be seen as a part of sensitivity analysis (Noble et al., 2017), meaning that we should run more than one publication bias test. Few simulation studies have explicitly investigated the performance of publication bias tests with non-independent data. Two simulation studies that we are aware of supported similar models to the multilevel-regression method we proposed here (Fernandez-Castilla et al., 2021; Rodgers & Pustejovsky, 2021). In addition, a general point to take from these two simulation studies is that most methods are prone to Type 2 error, with a possible exception of some selection models, even when the methods have nominal Type 1 error rates. Therefore, not detecting publication bias in a publication bias test should not be taken as a proof of no publication bias, including for multilevel regression. Clearly, we need more methodological and simulation-based work in the future. Finally, we repeat that the results of publication bias tests should always be cautiously interpreted because no methods will ever be able to verify the actual number of missing effect sizes (Nakagawa et al., 2022)
 
For meta-analysts to be able to assess publication bias, all empiricists need to report their statistical results, including their sample sizes and estimates of uncertainty (both SE and SD), transparently and compressively (Gerstner et al., 2017; Hennessy et al., 2021).

# Exploring sources of heterogeneity: assess the effect of including variables as moderators (multivariate)

Adding multiple moderators variables as fixed-effects leads to a multi-moderator multilevel meta-regression (mixed model)

The heterogeneity estimates for my model remain high despite accounting for sources of non-independence (p<0.001), therefore it is worthwhile assessing moderator effects to attempt to identify sources of heterogeneity. Potential moderators (and the expected level of their effects) were determined a priori. 

## 1.0 Add all biological/methodological moderator variables together (i.e. all non-reporting/study quality moderator variables)

```{r - convert all moderator variables to factors, results='hide'}
SMD.g <- SMD.g %>% 
  mutate(administration_route = as.factor(administration_route),
         sex_of_animals = as.factor(sex_of_animals), 
         species_of_animal = as.factor(species_of_animal), 
         developmental_stage_PPI = as.factor(developmental_stage_PPI), 
         strain = as.factor(strain), 
         gestational_stage_poly = as.factor(gestational_stage_poly))

```

```{r - add all variables with reference level set to largest level within the factor and including a species*strain interaction term, results='hide'}
ml_study.cohort.effect_re.VCV.5.bm <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                             mods = ~ relevel((administration_route), ref = 'iv') + relevel((sex_of_animals), ref = 'male') + poly_I_C_daily_dose_mg_kg + I(time) 
                                             + relevel((developmental_stage_PPI), ref = 'adult') + GD_first_administration + relevel((gestational_stage_poly), ref = 'mid-late gestation') +
                                               relevel((species_of_animal), ref = 'rat')*relevel((strain), ref = 'sprague-dawley'), data = SMD.g, method = 'REML')
                                             
                                             
summary(ml_study.cohort.effect_re.VCV.5.bm)
r2_ml(ml_study.cohort.effect_re.VCV.5.bm)
```
MODEL RESULTS 

Multivariate Meta-Analysis Model (k = 241; method: REML)

   logLik   Deviance        AIC        BIC       AICc   
-169.1350   338.2700   382.2700   457.1289   387.3554   

Variance Components:

            estim    sqrt  nlvls  fixed                         factor 
sigma^2.1  0.1647  0.4058     46     no                   study_id_str 
sigma^2.2  0.0687  0.2622     72     no            study_id_str/cohort 
sigma^2.3  0.0834  0.2887    241     no  study_id_str/cohort/effect_id 

Test for Residual Heterogeneity:
QE(df = 222) = 578.3345, p-val < .0001

Test of Moderators (coefficients 2:19):
QM(df = 18) = 39.5378, p-val = 0.0024

Model Results:

                                                                                  estimate      se     zval    pval    ci.lb    ci.ub     
intrcpt                                                                            -2.4072  2.4050  -1.0009  0.3169  -7.1209   2.3065     
relevel((administration_route), ref = "iv")ip                                       0.4642  0.2938   1.5798  0.1142  -0.1117   1.0400     
relevel((administration_route), ref = "iv")sc                                       0.5366  0.6020   0.8913  0.3727  -0.6433   1.7166     
relevel((sex_of_animals), ref = "male")female                                       0.0968  0.1763   0.5492  0.5829  -0.2488   0.4424     
relevel((sex_of_animals), ref = "male")mixed                                       -0.2520  0.2589  -0.9733  0.3304  -0.7595   0.2555     
poly_I_C_daily_dose_mg_kg                                                          -0.0710  0.0271  -2.6170  0.0089  -0.1241  -0.0178  ** 
I(time)                                                                            -0.0046  0.0029  -1.5847  0.1130  -0.0103   0.0011     
relevel((developmental_stage_PPI), ref = "adult")adolescent                         0.4925  0.2137   2.3049  0.0212   0.0737   0.9113   * 
relevel((developmental_stage_PPI), ref = "adult")juvenile                           0.1162  0.1596   0.7277  0.4668  -0.1967   0.4291     
GD_first_administration                                                             0.1818  0.1595   1.1402  0.2542  -0.1307   0.4944     
relevel((gestational_stage_poly), ref = "mid-late gestation")early-mid gestation    0.3257  0.8242   0.3951  0.6927  -1.2898   1.9411     
relevel((gestational_stage_poly), ref = "mid-late gestation")late gestation        -1.0156  0.5746  -1.7675  0.0771  -2.1417   0.1106   . 
relevel((species_of_animal), ref = "rat")mouse                                      0.4340  0.7771   0.5585  0.5765  -1.0891   1.9571     
relevel((strain), ref = "sprague-dawley")BALB/c                                     0.8000  1.0505   0.7616  0.4463  -1.2588   2.8589     
relevel((strain), ref = "sprague-dawley")C57BL/6                                    0.1264  0.7384   0.1711  0.8641  -1.3210   1.5737     
relevel((strain), ref = "sprague-dawley")C57BL/6J                                   0.1919  0.7638   0.2513  0.8016  -1.3052   1.6890     
relevel((strain), ref = "sprague-dawley")C57BL/6N                                   0.2318  0.7822   0.2963  0.7670  -1.3013   1.7648     
relevel((strain), ref = "sprague-dawley")long-evans                                -0.0709  0.3212  -0.2209  0.8252  -0.7005   0.5586     
relevel((strain), ref = "sprague-dawley")wistar                                    -0.5206  0.3451  -1.5085  0.1314  -1.1970   0.1558     

These moderators explain 37.0% of variance 
The moderators and random effects variables combined explain 83.4% of variance 

### 1.1 Get predicted values (estimate, CIs) for the individual levels of biological moderator variables  

```{r - predicted values for levels of biological moderators, results='hide'}
predicted_values_bm <- predict(ml_study.cohort.effect_re.VCV.5.bm, intercept=TRUE, addx=TRUE)
predicted_values_bm <- as.data.frame(predicted_values_bm) %>% 
  distinct()
write.csv(predicted_values_bm, 'pred_biol_method.csv')
```

```{r - read in formatted bio_meth multivariate predicted values (from excel), results='hide'}
pred_biol_method_formatted <- read.csv('pred_biol_method_formatted.csv')
```

```{r - make gt table for bio_meth multivariate predicted values}
pred_biol_method_formatted %>% 
  gt() %>% 
  cols_label(poly_I_C_daily_dose = md('daily dose of polyI:C (mg/kg)'), 
             administration_route = md('route of administration'), 
             GD_at_first_poly_I_C_administration = md('GDx of first polyI:C administration'), 
             gestational_stage_at_poly_I_C_administration = md('gestational stage at polyI:C administration'), 
             PND_of_PPI_testing = md('PNDx at PPI testing'), 
             developmental_stage_at_PPI_testing = md('developmental stage at PPI testing'), 
             ci_lb = md('95% CI lb'), 
             ci_ub = md('95% CI ub'),
             pi_lb = md('95% PI lb'), 
             pi_ub = md('95% PI ub')) %>% 
 opt_row_striping() %>% 
  tab_style(
     locations = cells_column_labels(columns = everything()),
     style     = list(
       #Give a thick border below
       cell_borders(sides = "bottom", weight = px(3)),
       #Make text bold
       cell_text(weight = "bold"))) %>% 
  tab_header(title = 'Estimated values and confidence intervals for the pooled effect sizes of subgroups of categorical biological and
             methodological variables',
             subtitle = 'calculated using multivariate multilevel random effects model fitted with biological and methodological
             variables as moderators')
```

## 2.0 Add all risk of bias moderator variables as moderators 
Random sequence generation, random outcome assessment, baseline characteristic reporting, random housing, allocation concealment, blinding of outcome assessment, blinding of animal handlers/carers, a priori power calculations, conflict of interest, unit of analysis error 

## 2.1 High and unclear risk of bias grouped together - less informative 

```{r - read in altered dataset with unclear and high RoB grouped together (now called high RoB), results='hide'}
SMD.g.bias <- read.csv('meta.unclear_to_high.csv')
SMD.g.bias <- SMD.g.bias %>% 
  mutate(ro_b_assessment_blinding_of_animal_carers_handlers = as.factor(ro_b_assessment_blinding_of_animal_carers_handlers), 
         ro_b_assessment_blinding_of_outcome_assessors = as.factor(ro_b_assessment_blinding_of_outcome_assessors), 
         ro_b_assessment_random_housing = as.factor(ro_b_assessment_random_housing), 
         ro_b_assessment_a_priori_power_calculations = as.factor(ro_b_assessment_a_priori_power_calculations), 
         ro_b_assessment_conflict_of_interest = as.factor(ro_b_assessment_conflict_of_interest), 
         ro_b_assessment_unit_of_analysis_error = as.factor(ro_b_assessment_unit_of_analysis_error), 
         ro_b_assessment_random_outcome_assessment = as.factor(ro_b_assessment_random_outcome_assessment), 
         ro_b_assessment_presence_of_sequence_generation = as.factor(ro_b_assessment_presence_of_sequence_generation), 
         ro_b_assessment_baseline_characteristics = as.factor(ro_b_assessment_baseline_characteristics), 
         ro_b_assessment_allocation_concealment = as.factor(ro_b_assessment_allocation_concealment), 
         ro_b_assessment_incomplete_outcome_data = as.factor(ro_b_assessment_incomplete_outcome_data))
```

```{r - fit model with risk of bias assessment criteria as moderator variables.group, results='hide'}
ml_study.cohort.effect_re.VCV.5.bias.group <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                             mods = ~ relevel((ro_b_assessment_blinding_of_animal_carers_handlers), ref = 'high RoB') + 
                                               relevel((ro_b_assessment_blinding_of_outcome_assessors), ref = 'high RoB') +
                                               relevel((ro_b_assessment_random_housing), ref = 'high RoB') +
                                               relevel((ro_b_assessment_a_priori_power_calculations), ref = 'high RoB') +
                                               relevel((ro_b_assessment_conflict_of_interest), ref = 'high RoB') + 
                                               relevel((ro_b_assessment_unit_of_analysis_error), ref = 'high RoB') + 
                                               relevel((ro_b_assessment_random_outcome_assessment), ref = 'high RoB') + 
                                               relevel((ro_b_assessment_presence_of_sequence_generation), ref = 'high RoB') + 
                                               relevel((ro_b_assessment_baseline_characteristics), ref = 'high RoB') + 
                                               relevel((ro_b_assessment_allocation_concealment), ref = 'high RoB') + 
                                               relevel((ro_b_assessment_incomplete_outcome_data), ref = 'high RoB'),
                                             data = SMD.g.bias, method = 'REML')

summary(ml_study.cohort.effect_re.VCV.5.bias.group)
r2_ml(ml_study.cohort.effect_re.VCV.5.bias.group)
```

Multivariate Meta-Analysis Model (k = 247; method: REML)

   logLik   Deviance        AIC        BIC       AICc   
-193.3188   386.6376   414.6376   463.1313   416.5381   

Variance Components:

            estim    sqrt  nlvls  fixed                         factor 
sigma^2.1  0.1906  0.4366     48     no                   study_id_str 
sigma^2.2  0.0837  0.2892     74     no            study_id_str/cohort 
sigma^2.3  0.0893  0.2989    247     no  study_id_str/cohort/effect_id 

Test for Residual Heterogeneity:
QE(df = 236) = 661.7218, p-val < .0001

Test of Moderators (coefficients 2:11):
QM(df = 10) = 13.4228, p-val = 0.2010

Model Results:

                                                                                        estimate      se     zval    pval    ci.lb   ci.ub    
intrcpt                                                                                  -0.3482  0.1970  -1.7674  0.0772  -0.7344  0.0379  . 
relevel((ro_b_assessment_blinding_of_animal_carers_handlers), ref = "high RoB")low RoB    0.1515  0.5268   0.2875  0.7737  -0.8811  1.1840    
relevel((ro_b_assessment_blinding_of_outcome_assessors), ref = "high RoB")low RoB         0.2830  0.3345   0.8460  0.3975  -0.3727  0.9387    
relevel((ro_b_assessment_random_housing), ref = "high RoB")low RoB                        1.1174  0.7869   1.4200  0.1556  -0.4249  2.6598    
relevel((ro_b_assessment_a_priori_power_calculations), ref = "high RoB")low RoB          -0.9039  0.8662  -1.0435  0.2967  -2.6017  0.7938    
relevel((ro_b_assessment_conflict_of_interest), ref = "high RoB")low RoB                  0.2576  0.2206   1.1677  0.2429  -0.1748  0.6900    
relevel((ro_b_assessment_unit_of_analysis_error), ref = "high RoB")low RoB               -0.2568  0.2037  -1.2605  0.2075  -0.6560  0.1425    
relevel((ro_b_assessment_random_outcome_assessment), ref = "high RoB")low RoB            -0.3683  0.2546  -1.4467  0.1480  -0.8672  0.1307    
relevel((ro_b_assessment_presence_of_sequence_generation), ref = "high RoB")low RoB      -0.3248  0.2297  -1.4144  0.1572  -0.7750  0.1253    
relevel((ro_b_assessment_baseline_characteristics), ref = "high RoB")low RoB              0.1387  0.3205   0.4328  0.6652  -0.4895  0.7669    
relevel((ro_b_assessment_incomplete_outcome_data), ref = "high RoB")low RoB              -0.1617  0.2043  -0.7916  0.4286  -0.5621  0.2387    

These moderators explain 20% of variance 

## 2.2 Unclear and high RoB kept as separate levels - more informative 

```{r - turn risk of bias moderators into factors}
SMD.g <- SMD.g %>% 
  mutate(ro_b_assessment_blinding_of_animal_carers_handlers = as.factor(ro_b_assessment_blinding_of_animal_carers_handlers), 
         ro_b_assessment_blinding_of_outcome_assessors = as.factor(ro_b_assessment_blinding_of_outcome_assessors), 
         ro_b_assessment_random_housing = as.factor(ro_b_assessment_random_housing), 
         ro_b_assessment_a_priori_power_calculations = as.factor(ro_b_assessment_a_priori_power_calculations), 
         ro_b_assessment_conflict_of_interest = as.factor(ro_b_assessment_conflict_of_interest), 
         ro_b_assessment_unit_of_analysis_error = as.factor(ro_b_assessment_unit_of_analysis_error), 
         ro_b_assessment_random_outcome_assessment = as.factor(ro_b_assessment_random_outcome_assessment), 
         ro_b_assessment_presence_of_sequence_generation = as.factor(ro_b_assessment_presence_of_sequence_generation), 
         ro_b_assessment_baseline_characteristics = as.factor(ro_b_assessment_baseline_characteristics), 
         ro_b_assessment_allocation_concealment = as.factor(ro_b_assessment_allocation_concealment), 
         ro_b_assessment_incomplete_outcome_data = as.factor(ro_b_assessment_incomplete_outcome_data))
```

```{r - fit model with risk of bias assessment criteria as moderator variables.sep, results='hide'}
ml_study.cohort.effect_re.VCV.5.bias.sep <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                             mods = ~ relevel((ro_b_assessment_blinding_of_animal_carers_handlers), ref = 'unclear RoB') + 
                                               relevel((ro_b_assessment_blinding_of_outcome_assessors), ref = 'unclear RoB') +
                                               relevel((ro_b_assessment_random_housing), ref = 'unclear RoB') +
                                               relevel((ro_b_assessment_a_priori_power_calculations), ref = 'unclear RoB') +
                                               relevel((ro_b_assessment_conflict_of_interest), ref = 'low RoB') + 
                                               relevel((ro_b_assessment_unit_of_analysis_error), ref = 'high RoB') + 
                                               relevel((ro_b_assessment_random_outcome_assessment), ref = 'unclear RoB') + 
                                               relevel((ro_b_assessment_presence_of_sequence_generation), ref = 'unclear RoB') + 
                                               relevel((ro_b_assessment_baseline_characteristics), ref = 'unclear RoB') + 
                                               relevel((ro_b_assessment_allocation_concealment), ref = 'unclear RoB') + 
                                               relevel((ro_b_assessment_incomplete_outcome_data), ref = 'low RoB'),
                                             data = SMD.g, method = 'REML')

summary(ml_study.cohort.effect_re.VCV.5.bias.sep)
r2_ml(ml_study.cohort.effect_re.VCV.5.bias.sep)
```
Multivariate Meta-Analysis Model (k = 247; method: REML)

   logLik   Deviance        AIC        BIC       AICc   
-184.4349   368.8697   406.8697   472.2757   410.4716   

Variance Components:

            estim    sqrt  nlvls  fixed                         factor 
sigma^2.1  0.1648  0.4059     48     no                   study_id_str 
sigma^2.2  0.0840  0.2898     74     no            study_id_str/cohort 
sigma^2.3  0.0897  0.2995    247     no  study_id_str/cohort/effect_id 

Test for Residual Heterogeneity:
QE(df = 231) = 623.3682, p-val < .0001

Test of Moderators (coefficients 2:16):
QM(df = 15) = 19.9928, p-val = 0.1722

Model Results:

                                                                                            estimate      se     zval    pval    ci.lb   ci.ub    
intrcpt                                                                                      -0.4187  0.2526  -1.6578  0.0974  -0.9138  0.0763  . 
relevel((ro_b_assessment_blinding_of_animal_carers_handlers), ref = "unclear RoB")high RoB    0.1660  0.6858   0.2420  0.8088  -1.1782  1.5102    
relevel((ro_b_assessment_blinding_of_animal_carers_handlers), ref = "unclear RoB")low RoB     0.0905  0.5415   0.1672  0.8672  -0.9708  1.1519    
relevel((ro_b_assessment_blinding_of_outcome_assessors), ref = "unclear RoB")low RoB          0.3107  0.3304   0.9405  0.3470  -0.3368  0.9583    
relevel((ro_b_assessment_random_housing), ref = "unclear RoB")high RoB                       -0.6917  0.7066  -0.9789  0.3276  -2.0765  0.6932    
relevel((ro_b_assessment_random_housing), ref = "unclear RoB")low RoB                         0.9705  0.7814   1.2420  0.2142  -0.5610  2.5019    
relevel((ro_b_assessment_a_priori_power_calculations), ref = "unclear RoB")low RoB           -1.1067  0.8605  -1.2861  0.1984  -2.7933  0.5798    
relevel((ro_b_assessment_conflict_of_interest), ref = "unclear RoB")high RoB                 -0.7147  0.4275  -1.6719  0.0945  -1.5525  0.1231  . 
relevel((ro_b_assessment_conflict_of_interest), ref = "unclear RoB")low RoB                   0.1927  0.2362   0.8158  0.4146  -0.2703  0.6558    
relevel((ro_b_assessment_unit_of_analysis_error), ref = "high RoB")low RoB                   -0.2450  0.2068  -1.1847  0.2361  -0.6503  0.1603    
relevel((ro_b_assessment_unit_of_analysis_error), ref = "high RoB")unclear RoB               -1.0670  0.7973  -1.3384  0.1808  -2.6296  0.4956    
relevel((ro_b_assessment_random_outcome_assessment), ref = "unclear RoB")low RoB             -0.5305  0.2925  -1.8138  0.0697  -1.1038  0.0428  . 
relevel((ro_b_assessment_presence_of_sequence_generation), ref = "unclear RoB")low RoB       -0.1878  0.2660  -0.7059  0.4803  -0.7091  0.3336    
relevel((ro_b_assessment_baseline_characteristics), ref = "unclear RoB")low RoB               0.2393  0.3599   0.6648  0.5062  -0.4661  0.9446    
relevel((ro_b_assessment_incomplete_outcome_data), ref = "low RoB")high RoB                   0.2095  0.2711   0.7729  0.4396  -0.3218  0.7408    
relevel((ro_b_assessment_incomplete_outcome_data), ref = "low RoB")unclear RoB                0.1335  0.2314   0.5770  0.5639  -0.3201  0.5872    

Inclusion of these variables as fixed effects (moderators) explains 29% of the variance 
Inclusion of these variables as fixed effects (moderators) in addition to the random effects terms explains 81.2% of the variance 

### 2.1 Get predicted values (estimate, CIs) for the individual levels of risk of bias moderator 

```{r - get full multivariate predictions for bias moderators, results = 'hide'}
predicted_values_bias <- predict.rma(ml_study.cohort.effect_re.VCV.5.bias.sep, intercept=TRUE, addx=TRUE)
predicted_values_bias <- as.data.frame(predicted_values_bias) %>% 
  distinct()
write.csv(predicted_values_bias, 'pred_bias.csv')
```

```{r - read in formatted csv of multivariate predicted values for bias moderators}
pred_bias_formatted <- read.csv('pred_bias_formatted.csv')
```

```{r - make gt table for bias multivariate predicted values}
pred_bias_formatted %>% 
  gt() %>% 
  cols_label(ap.power.calc = md('a priori power calculations'), 
             presence.of.sequence.generation = md('random sequence generation'), 
             reporting.of.baseline.characteristics = md('reporting of baseline characteristics'), 
             blinding.animal.carers.handlers = md('blinding of animal carers/handlers'), 
             random.housing = md('random housing'), 
             random.outcome.assessment = md('random outcome assessment'), 
             blinded.outcome.assessment = md('blinded outcome assessment'), 
             uoa.error = md('unit of analysis error'), 
             coi = md('conflict of interest')) %>% 
 opt_row_striping() %>% 
  tab_style(
     locations = cells_column_labels(columns = everything()),
     style     = list(
       #Give a thick border below
       cell_borders(sides = "bottom", weight = px(3)),
       #Make text bold
       cell_text(weight = "bold"))) %>% 
  tab_header(title = 'Estimated values and confidence intervals for the pooled effect sizes of studies with low, unclear and high risks
             of bias for the SYRCLE risk of bias assessment criteria',
             subtitle = 'calculated using multivariate multilevel random effects model fitted with risk of bias variables as moderators')
```

## 3.0 Future: teasing out how much heterogeneity is due to disease model induction protocol vs PPI measurement protocol 
This could be done by assessing variation from only moderators relating to PPI testing protocol (but would require larger dataset to get enough studies with %PPI values for specific pre-pulses/pulses/ITIs), and then repeating but for moderator variables only relating to polyI:C protocol induction. This would be hard though because would need to extract additional information like polyI:C manufacturer, polyI:C molecular weight etc. for the analysis to be of any relevance. 

*does this make any sense at all or would it make a statistician scream?*
Could it also be done by calculating lnCV for polyI:C cohort (let this be x), and then lnCV for control cohort (let this be y) and comparing the two? Would it be logical that x-y = the variability attributed to disease model induction protocol? 

# Exploring sources of heterogeneity: assess the effect of including variables as moderators (univariate)

## 1. Biological/methodological variables 

### 1.1 Admnistration route (univariate)
```{r - fit univariate model with administration route as moderator and predict values, results='hide', echo=FALSE}
ml_study.cohort.effect_re.VCV.5.administration_route <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                               mods = ~ relevel((administration_route), ref = 'iv'), data = SMD.g, method = 'REML')

administration_route_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.administration_route, intercept = TRUE, addx = TRUE)
administration_route_uvpv <- as.data.frame(administration_route_uvpv) %>% 
  distinct()
```

```{r - univariate predicted values for iv administration route, results='hide',echo=FALSE}
administration_route_uvpv %>% 
  filter(X.relevel..administration_route...ref....iv..ip == '0') %>% 
  filter(X.relevel..administration_route...ref....iv..sc == '0')
```

```{r - univariate predicted values for ip administration route, results='hide', echo=FALSE}
administration_route_uvpv %>% 
  filter(X.relevel..administration_route...ref....iv..ip == '1') 
```

```{r - univariate predicted values for sc administration route, results='hide', echo=FALSE}
administration_route_uvpv %>% 
  filter(X.relevel..administration_route...ref....iv..sc == '1') 
```

### 1.1.2 Sex of animals (univariate)

```{r - fit univariate model with sex as moderator and predict values, results='hide', echo=FALSE}
ml_study.cohort.effect_re.VCV.5.sex <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                               mods = ~ relevel((sex_of_animals), ref = 'male'), data = SMD.g, method = 'REML')

sex_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.sex, intercept = TRUE, addx = TRUE)
sex_uvpv <- as.data.frame(sex_uvpv) %>% 
  distinct()
```

```{r - univariate predicted values for male cohorts, results='hide', echo=FALSE}
sex_uvpv %>% 
  filter(X.relevel..sex_of_animals...ref....male..female. == '0') %>% 
  filter(X.relevel..sex_of_animals...ref....male..mixed. == '0')
```

```{r - univariate predicted values for female cohorts, results='hide', echo=FALSE}
sex_uvpv %>% 
  filter(X.relevel..sex_of_animals...ref....male..female. == '1') 
```

```{r - univariate predicted values for mixed cohorts, results='hide', echo=FALSE}
sex_uvpv %>% 
  filter(X.relevel..sex_of_animals...ref....male..mixed. == '1')
```

### 1.1.3 Developmental stage of animal at PPI testing (univariate) 

```{r - fit univariate model with developmental stage of animal as moderator and predict values, results='hide', echo=FALSE}
ml_study.cohort.effect_re.VCV.5.dev_stage <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                               mods = ~ relevel((developmental_stage_PPI), ref = 'adult'), data = SMD.g, method = 'REML')

dev_stage_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.dev_stage, intercept = TRUE, addx = TRUE)
dev_stage_uvpv <- as.data.frame(dev_stage_uvpv) %>% 
  distinct()
```

```{r - univariate predicted values for adult offspring, results='hide', echo=FALSE}
dev_stage_uvpv %>% 
  filter(X.relevel..developmental_stage_PPI...ref....adult..adolescent == '0') %>% 
  filter(X.relevel..developmental_stage_PPI...ref....adult..juvenile == '0')
```

```{r - univariate predicted values for adolescent offspring, results='hide', echo=FALSE}
dev_stage_uvpv %>% 
  filter(X.relevel..developmental_stage_PPI...ref....adult..adolescent == '1') 
```

```{r - univariate predicted values for juvenile offspring, results='hide', echo=FALSE}
dev_stage_uvpv %>% 
  filter(X.relevel..developmental_stage_PPI...ref....adult..juvenile == '1')
```


### 1.1.4 Gestational stage of animal at time of polyI:C administration (univariate) 

```{r - fit univariate model with gestational stage of dam as moderator and predict values, results='hide', echo=FALSE}
ml_study.cohort.effect_re.VCV.5.gest_stage <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                               mods = ~ relevel((gestational_stage_poly), ref = 'mid-late gestation'), data = SMD.g, method = 'REML')

gest_stage_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.gest_stage, intercept = TRUE, addx = TRUE)
gest_stage_uvpv <- as.data.frame(gest_stage_uvpv) %>% 
  distinct()
```

```{r - univariate predicted values for mid-late gestation, results='hide', echo=FALSE}
gest_stage_uvpv %>% 
  filter(X.relevel..gestational_stage_poly...ref....mid.late.gestation..early.mid.gestation == '0') %>% 
  filter(X.relevel..gestational_stage_poly...ref....mid.late.gestation..late.gestation == '0')
```

```{r - univariate predicted values for early-mid gestation, results='hide', echo=FALSE}
gest_stage_uvpv %>% 
  filter(X.relevel..gestational_stage_poly...ref....mid.late.gestation..early.mid.gestation == '1') 
```

```{r - univariate predicted values for late gestation, results='hide', echo=FALSE}
gest_stage_uvpv %>% 
  filter(X.relevel..gestational_stage_poly...ref....mid.late.gestation..late.gestation == '1')
```

### 1.1.5 Species of animal (univariate) 

```{r - fit univariate model with species as moderator and predict values, results='hide', echo=FALSE}
ml_study.cohort.effect_re.VCV.5.species <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                               mods = ~ relevel((species_of_animal), ref = 'rat'), data = SMD.g, method = 'REML')

species_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.species, intercept = TRUE, addx = TRUE)
species_uvpv <- as.data.frame(species_uvpv) %>% 
  distinct()
```

```{r - univariate predicted values for rat cohorts, results='hide', echo=FALSE}
species_uvpv %>% 
  filter(X.relevel..species_of_animal...ref....rat..mouse == '0')
```

```{r - univariate predicted values for mouse cohorts, results='hide', echo=FALSE}
species_uvpv %>% 
  filter(X.relevel..species_of_animal...ref....rat..mouse == '1')
```

### 1.1.6 Strain of animal (univariate) 

```{r - fit univariate model with strain as moderator and predict values, results='hide', echo=FALSE}
ml_study.cohort.effect_re.VCV.5.strain <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                               mods = ~ relevel((strain), ref = 'sprague-dawley'), data = SMD.g, method = 'REML')

strain_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.strain, intercept = TRUE, addx = TRUE)
strain_uvpv <- as.data.frame(strain_uvpv) %>% 
  distinct()
```

```{r - univariate predicted values for sprague-dawley cohorts, results='hide', echo=FALSE}
strain_uvpv %>% 
  filter(X.relevel..strain...ref....sprague.dawley..BALB.c == '0') %>% 
  filter(X.relevel..strain...ref....sprague.dawley..C57BL.6 == '0') %>% 
  filter(X.relevel..strain...ref....sprague.dawley..C57BL.6J == '0') %>% 
  filter(X.relevel..strain...ref....sprague.dawley..C57BL.6N == '0') %>% 
  filter(X.relevel..strain...ref....sprague.dawley..ddY == '0') %>% 
  filter(X.relevel..strain...ref....sprague.dawley..long.evans == '0') %>% 
  filter(X.relevel..strain...ref....sprague.dawley..wistar == '0') %>% 
  filter(X.relevel..strain...ref....sprague.dawley..wistar.hannover == '0') 
```

```{r - univariate predicted values for BALB/c cohorts, results='hide', echo=FALSE}
strain_uvpv %>% 
  filter(X.relevel..strain...ref....sprague.dawley..BALB.c == '1') 
```

```{r - univariate predicted values for C57BL/6 cohorts, results='hide', echo=FALSE}
strain_uvpv %>% 
  filter(X.relevel..strain...ref....sprague.dawley..C57BL.6 == '1') 
```

```{r - univariate predicted values for C57BL/6J cohorts, results='hide', echo=FALSE}
strain_uvpv %>% 
  filter(X.relevel..strain...ref....sprague.dawley..C57BL.6J == '1') 
```

```{r - univariate predicted values for C57BL/6N cohorts, results='hide', echo=FALSE}
strain_uvpv %>% 
  filter(X.relevel..strain...ref....sprague.dawley..C57BL.6N == '1') 
```

```{r - univariate predicted values for ddY cohorts, results='hide', echo=FALSE}
strain_uvpv %>% 
  filter(X.relevel..strain...ref....sprague.dawley..ddY == '1') 
```

```{r - univariate predicted values for long evans cohorts, results='hide', echo=FALSE}
strain_uvpv %>% 
  filter(X.relevel..strain...ref....sprague.dawley..long.evans == '1') 
```

```{r - univariate predicted values for wistar cohorts, results='hide', echo=FALSE}
strain_uvpv %>% 
  filter(X.relevel..strain...ref....sprague.dawley..wistar == '1') 
```

```{r - univariate predicted values for wistar-hannover cohorts, results='hide', echo=FALSE}
strain_uvpv %>% 
  filter(X.relevel..strain...ref....sprague.dawley..wistar.hannover == '1') 
```

## Read in csv file with univariate predicted results for biological moderator variables 

```{r - read in csv with univariate biological moderator predicted values, results='hide'}
univariate_moderator_predictions <- read.csv('univariate_moderator_predictions.csv')
univariate_moderator_predictions <- univariate_moderator_predictions %>% 
  mutate(Moderator = as.factor(Moderator))

univariate_moderator_predictions$Moderator <- fct_relevel(univariate_moderator_predictions$Moderator, 'Species', 'Strain', 'Sex of cohort', 'Administration route', 'Gestational stage at polyIC injection', 'Developmental stage at PPI testing ')

```

## Forest plot 

```{r - forest plot of univariate predicted results for biological moderator variables}
bio_mods_plot <- univariate_moderator_predictions %>% 
  mutate(Level = factor(Level, levels = unique(Level))) %>% 
  group_by(Moderator)%>%
  ggplot(aes(x = Level, y= SMD, ymin=l95, ymax=u95, shape = shape))+
  scale_x_discrete(limits=rev) +
  scale_y_continuous(name="Standardised Mean Difference", limits=(c(-2,2)))+
  expand_limits(y=c(-2, 2))+
  geom_pointrange() +
  geom_hline(yintercept =0, linetype=2)+
  geom_errorbar(aes(ymin=l95, ymax=u95),width=0.05, cex=0)+
  theme_minimal()+
  coord_flip()+
  facet_wrap(~ Moderator, strip.position="left", labeller = label_wrap_gen(width=20), nrow=23, scales = "free_y")+
  geom_rect(aes(fill = Moderator),xmin = -Inf,xmax = Inf,
            ymin = -Inf,ymax = Inf,alpha = 0.1) +
  geom_point(shape=1, size=0.3)+
  labs(title = 'Estimated values for biological and methodological moderator variables')+
  theme(plot.title=element_text(size=16,face="bold"),
        legend.position = "none",
        axis.text.y=element_text(),
        axis.ticks.y=element_blank(),
        axis.text.x=element_text(face="bold", ),
        axis.title.x =element_text(size=10,face="bold"),
        axis.title.y = element_blank(),
        strip.text.y.left = element_text(hjust=0,vjust = 0.5,angle=0,face="bold", size=10),
        strip.placement = "outside")

bio_mods_plot
```

             
## 2. Risk of bias moderator variables 

```{r - mutate RoB variables to be factors, results='hide'}
SMD.g <- SMD.g %>% 
  mutate(ro_b_assessment_blinding_of_animal_carers_handlers = as.factor(ro_b_assessment_blinding_of_animal_carers_handlers), 
         ro_b_assessment_blinding_of_outcome_assessors = as.factor(ro_b_assessment_blinding_of_outcome_assessors), 
         ro_b_assessment_random_housing = as.factor(ro_b_assessment_random_housing), 
         ro_b_assessment_a_priori_power_calculations = as.factor(ro_b_assessment_a_priori_power_calculations), 
         ro_b_assessment_conflict_of_interest = as.factor(ro_b_assessment_conflict_of_interest), 
         ro_b_assessment_unit_of_analysis_error = as.factor(ro_b_assessment_unit_of_analysis_error), 
         ro_b_assessment_random_outcome_assessment = as.factor(ro_b_assessment_random_outcome_assessment), 
         ro_b_assessment_presence_of_sequence_generation = as.factor(ro_b_assessment_presence_of_sequence_generation), 
         ro_b_assessment_baseline_characteristics = as.factor(ro_b_assessment_baseline_characteristics), 
         ro_b_assessment_allocation_concealment = as.factor(ro_b_assessment_allocation_concealment), 
         ro_b_assessment_incomplete_outcome_data = as.factor(ro_b_assessment_incomplete_outcome_data))
```

### 2.1 Blinding of animal handlers (univariate)
```{r - fit model with blinding of animal handlers as moderator variable, results='hide', echo=FALSE}
ml_study.cohort.effect_re.VCV.5.bias.blindhand <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                         mods = ~ relevel((ro_b_assessment_blinding_of_animal_carers_handlers), 
                                                                          ref = 'unclear RoB'), 
                                                         data = SMD.g, method = 'REML')

blindhand_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.bias.blindhand, intercept=TRUE, addx=TRUE)
blindhand_uvpv <- blindhand_uvpv %>% 
  as.data.frame() %>% 
  distinct()
```

```{r - univariate predicted values for unclear RoB blinding of animal handlers, results='hide'}
blindhand_uvpv %>% 
  filter(X.relevel..ro_b_assessment_blinding_of_animal_carers_handlers...ref....unclear.RoB..high.RoB == '0', 
         X.relevel..ro_b_assessment_blinding_of_animal_carers_handlers...ref....unclear.RoB..low.RoB == '0')
```

```{r - univariate predicted values for high RoB blinding of animal handlers, results='hide'}
blindhand_uvpv %>% 
  filter(X.relevel..ro_b_assessment_blinding_of_animal_carers_handlers...ref....unclear.RoB..high.RoB == '1')
```

```{r - univariate predicted values for low RoB blinding of animal handlers, results='hide'}
blindhand_uvpv %>% 
  filter(X.relevel..ro_b_assessment_blinding_of_animal_carers_handlers...ref....unclear.RoB..low.RoB == '1')
```

### 2.2 Blinding of outcome assessors (univariate) 

```{r - fit model with blinding of outcome assessors as moderator variable, results='hide'}
ml_study.cohort.effect_re.VCV.5.bias.blindout <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                         mods = ~ relevel((ro_b_assessment_blinding_of_outcome_assessors), 
                                                                          ref = 'unclear RoB'), 
                                                         data = SMD.g, method = 'REML')

blindout_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.bias.blindout, intercept=TRUE, addx=TRUE)
blindout_uvpv <- blindout_uvpv %>% 
  as.data.frame() %>% 
  distinct()
```

```{r - univariate predicted values for unclear RoB blinding of outcome assessors, results='hide'}
blindout_uvpv %>% 
  filter(X.relevel..ro_b_assessment_blinding_of_outcome_assessors...ref....unclear.RoB..low.RoB == '0') %>%
  filter(X.relevel..ro_b_assessment_blinding_of_outcome_assessors...ref....unclear.RoB..high.RoB == '0')
```

```{r - univariate predicted values for low RoB blinding of outcome assessors, results='hide'}
blindout_uvpv %>% 
  filter(X.relevel..ro_b_assessment_blinding_of_outcome_assessors...ref....unclear.RoB..low.RoB == '1') 
```

```{r - univariate predicted values for high RoB blinding of outcome assessors, results='hide'}
blindout_uvpv %>% 
  filter(X.relevel..ro_b_assessment_blinding_of_outcome_assessors...ref....unclear.RoB..high.RoB == '1')
```

### 2.3 Random housing of animals (univariate)

```{r - fit model with random housing of animals as moderator variable, results='hide'}
ml_study.cohort.effect_re.VCV.5.bias.randhous <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                         mods = ~ relevel((ro_b_assessment_random_housing), 
                                                                          ref = 'unclear RoB'), 
                                                         data = SMD.g, method = 'REML')

randhous_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.bias.randhous, intercept=TRUE, addx=TRUE)
randhous_uvpv <- randhous_uvpv %>% 
  as.data.frame() %>% 
  distinct()
```

```{r - univariate predicted values for unclear RoB random housing, results='hide'}
randhous_uvpv %>% 
  filter(X.relevel..ro_b_assessment_random_housing...ref....unclear.RoB..low.RoB == '0') %>% 
  filter(X.relevel..ro_b_assessment_random_housing...ref....unclear.RoB..high.RoB == '0')
```

```{r - univariate predicted values for high RoB random housing, results='hide'}
randhous_uvpv %>% 
  filter(X.relevel..ro_b_assessment_random_housing...ref....unclear.RoB..high.RoB == '1') 
```

```{r - univariate predicted values for low RoB random housing, results='hide'}
randhous_uvpv %>% 
  filter(X.relevel..ro_b_assessment_random_housing...ref....unclear.RoB..low.RoB == '1')
```

### 2.4 A priori power calculations (univariate)

```{r - fit model with a priori power calculations as moderator variable, results='hide'}
ml_study.cohort.effect_re.VCV.5.bias.appowercalc <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                         mods = ~ relevel((ro_b_assessment_a_priori_power_calculations), 
                                                                          ref = 'unclear RoB'), 
                                                         data = SMD.g, method = 'REML')

powercalc_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.bias.appowercalc, intercept=TRUE, addx=TRUE)
powercalc_uvpv <- powercalc_uvpv %>% 
  as.data.frame() %>% 
  distinct()
```

```{r - univariate predicted values for unclear RoB a priori power calc, results='hide'}
powercalc_uvpv %>% 
  filter(X.relevel..ro_b_assessment_a_priori_power_calculations...ref....unclear.RoB..low.RoB == '0')
```

```{r - univariate predicted values for low RoB a priori power calc, results='hide'}
powercalc_uvpv %>% 
  filter(X.relevel..ro_b_assessment_a_priori_power_calculations...ref....unclear.RoB..low.RoB == '1')
```

### 2.5 Conflict of interest (univariate)

```{r - fit model with COI as moderator variable, results='hide'}
ml_study.cohort.effect_re.VCV.5.bias.coi <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                         mods = ~ relevel((ro_b_assessment_conflict_of_interest), 
                                                                          ref = 'unclear RoB'), 
                                                         data = SMD.g, method = 'REML')

coi_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.bias.coi, intercept=TRUE, addx=TRUE)
coi_uvpv <- coi_uvpv %>% 
  as.data.frame() %>% 
  distinct()
```

```{r - univariate predicted values for unclear RoB COI, results='hide'}
coi_uvpv %>% 
  filter(X.relevel..ro_b_assessment_conflict_of_interest...ref....unclear.RoB..high.RoB == '0') %>% 
  filter(X.relevel..ro_b_assessment_conflict_of_interest...ref....unclear.RoB..low.RoB == '0')
```

```{r - univariate predicted values for high RoB COI, results='hide'}
coi_uvpv %>% 
  filter(X.relevel..ro_b_assessment_conflict_of_interest...ref....unclear.RoB..high.RoB == '1') 
```

```{r - univariate predicted values for low RoB COI, results='hide'}
coi_uvpv %>% 
  filter(X.relevel..ro_b_assessment_conflict_of_interest...ref....unclear.RoB..low.RoB == '1')
```

### 2.6 Unit of analysis error (univariate) high

```{r - fit model with uoa error as moderator variable, results='hide'}
ml_study.cohort.effect_re.VCV.5.bias.uoaerror <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                         mods = ~ relevel((ro_b_assessment_unit_of_analysis_error), 
                                                                          ref = 'high RoB'), 
                                                         data = SMD.g, method = 'REML')

uoaerror_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.bias.uoaerror, intercept=TRUE, addx=TRUE)
uoaerror_uvpv <- uoaerror_uvpv %>% 
  as.data.frame() %>% 
  distinct()
```

```{r - univariate predicted values for unclear RoB unit of analysis error}
uoaerror_uvpv %>% 
  filter(X.relevel..ro_b_assessment_unit_of_analysis_error...ref....high.RoB..unclear.RoB == '1')
```

```{r - univariate predicted values for high RoB unit of analysis error}
uoaerror_uvpv %>% 
  filter(X.relevel..ro_b_assessment_unit_of_analysis_error...ref....high.RoB..unclear.RoB == '0') %>% 
  filter(X.relevel..ro_b_assessment_unit_of_analysis_error...ref....high.RoB..low.RoB == '0')
```

```{r - univariate predicted values for low RoB unit of analysis error}
uoaerror_uvpv %>% 
  filter(X.relevel..ro_b_assessment_unit_of_analysis_error...ref....high.RoB..low.RoB == '1')
```

### 2.7 Random outcome assessment (univariate)

```{r - fit model with random outcome assessment as moderator variable, results='hide'}
ml_study.cohort.effect_re.VCV.5.bias.randout <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                         mods = ~ relevel((ro_b_assessment_random_outcome_assessment), 
                                                                          ref = 'unclear RoB'), 
                                                         data = SMD.g, method = 'REML')

randout_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.bias.randout, intercept=TRUE, addx=TRUE)
randout_uvpv <- randout_uvpv %>% 
  as.data.frame() %>% 
  distinct()
```

```{r - univariate predicted values for unclear RoB random outcome assessment} 
randout_uvpv %>% 
  filter(X.relevel..ro_b_assessment_random_outcome_assessment...ref....unclear.RoB..low.RoB == '0')
```

```{r - univariate predicted values for low RoB random outcome assessment} 
randout_uvpv %>% 
  filter(X.relevel..ro_b_assessment_random_outcome_assessment...ref....unclear.RoB..low.RoB == '1')
```

### 2.8 Presence of sequence generation (univariate)

```{r - fit model with presence of sequence generation as moderator variable, results='hide'}
ml_study.cohort.effect_re.VCV.5.bias.sequencegen <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                         mods = ~ relevel((ro_b_assessment_presence_of_sequence_generation), 
                                                                          ref = 'unclear RoB'), 
                                                         data = SMD.g, method = 'REML')

seqgen_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.bias.sequencegen, intercept=TRUE, addx=TRUE)
seqgen_uvpv <- seqgen_uvpv %>% 
  as.data.frame() %>% 
  distinct()
```

```{r - univariate predicted values for unclear RoB presence of sequence generation}
seqgen_uvpv %>% 
  filter(X.relevel..ro_b_assessment_presence_of_sequence_generation...ref....unclear.RoB..low.RoB == '0')
```

```{r - univariate predicted values for low RoB presence of sequence generation}
seqgen_uvpv %>% 
  filter(X.relevel..ro_b_assessment_presence_of_sequence_generation...ref....unclear.RoB..low.RoB == '1')
```

### 2.9 Reporting of baseline characteristics (univariate) 

```{r - fit model with reporting of baseline characteristics of animals as moderator variable, results='hide'}
ml_study.cohort.effect_re.VCV.5.bias.baselinechar <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                         mods = ~ relevel((ro_b_assessment_baseline_characteristics), 
                                                                          ref = 'unclear RoB'), 
                                                         data = SMD.g, method = 'REML')

basechar_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.bias.baselinechar, intercept=TRUE, addx=TRUE)
basechar_uvpv <- basechar_uvpv %>% 
  as.data.frame() %>% 
  distinct()
```

```{r - univariate predicted values for unclear RoB baseline characteristics}
basechar_uvpv %>% 
  filter(X.relevel..ro_b_assessment_baseline_characteristics...ref....unclear.RoB..low.RoB == '0')
```

```{r - univariate predicted values for low RoB baseline characteristics}
basechar_uvpv %>% 
  filter(X.relevel..ro_b_assessment_baseline_characteristics...ref....unclear.RoB..low.RoB == '1')
```

### 2.10 Allocation concealement (univariate)

```{r - fit model with allocation concealment as moderator variable, results='hide'}
ml_study.cohort.effect_re.VCV.5.bias.allocconc <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                         mods = ~ relevel((ro_b_assessment_allocation_concealment), 
                                                                          ref = 'unclear RoB'), 
                                                         data = SMD.g, method = 'REML')

allocconc_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.bias.allocconc, intercept=TRUE, addx=TRUE)
allocconc_uvpv <- allocconc_uvpv %>% 
  as.data.frame() %>% 
  distinct()
```

```{r - univariate predicted vales for unclear RoB allocation concealment}
allocconc_uvpv %>% 
  filter(X.relevel..ro_b_assessment_allocation_concealment...ref....unclear.RoB..high.RoB == '0') %>% 
  filter(X.relevel..ro_b_assessment_allocation_concealment...ref....unclear.RoB..low.RoB == '0')
```

```{r - univariate predicted vales for high RoB allocation concealment}
allocconc_uvpv %>% 
  filter(X.relevel..ro_b_assessment_allocation_concealment...ref....unclear.RoB..high.RoB == '1') 
```

```{r - univariate predicted vales for low RoB allocation concealment}
allocconc_uvpv %>% 
  filter(X.relevel..ro_b_assessment_allocation_concealment...ref....unclear.RoB..low.RoB == '1')
```

### 2.11 Incomplete outcome data (univariate) low

```{r - fit model with incomplete outcome data as moderator variable, results='hide'}
ml_study.cohort.effect_re.VCV.5.bias.incompdat <- rma.mv(yi = SMD, V = VCV.5, random = ~1 | study_id_str / cohort / effect_id, 
                                                         mods = ~ relevel((ro_b_assessment_incomplete_outcome_data), 
                                                                          ref = 'low RoB'), 
                                                         data = SMD.g, method = 'REML')

incompdat_uvpv <- predict.rma(ml_study.cohort.effect_re.VCV.5.bias.incompdat, intercept=TRUE, addx=TRUE)
incompdat_uvpv <- incompdat_uvpv %>% 
  as.data.frame() %>% 
  distinct()
```

```{r - univariate predicted values for unclear RoB incomplete outcome data}
incompdat_uvpv %>% 
  filter(X.relevel..ro_b_assessment_incomplete_outcome_data...ref....low.RoB..unclear.RoB == '1')
```

```{r - univariate predicted values for high RoB incomplete outcome data}
incompdat_uvpv %>% 
  filter(X.relevel..ro_b_assessment_incomplete_outcome_data...ref....low.RoB..high.RoB == '1')
```

```{r - univariate predicted values for low RoB incomplete outcome data}
incompdat_uvpv %>% 
  filter(X.relevel..ro_b_assessment_incomplete_outcome_data...ref....low.RoB..unclear.RoB == '0') %>% 
  filter(X.relevel..ro_b_assessment_incomplete_outcome_data...ref....low.RoB..high.RoB == '0')
```

## Read in csv file with univariate predicted results for risk of bias moderator variables 

```{r - read in csv with univariate risk of bias moderator predicted values, results='hide'}
univariate_moderator_predictions_bias <- read.csv('univariate_moderator_predictions_bias.csv')
univariate_moderator_predictions_bias <- univariate_moderator_predictions_bias %>% 
  mutate(Moderator = as.factor(Moderator))

univariate_moderator_predictions_bias$Moderator <- fct_relevel(univariate_moderator_predictions_bias$Moderator, 
                                                          'Presence of random sequence generation', 'Allocation concealment ', 
                                                          'Blinding of animal handlers/carers', 'Reporting of baseline characteristics ',
                                                          'Random housing of animals ', 'Random outcome assessment ', 
                                                          'Blinding of outcome assessors ', 'Incomplete outcome data ',
                                                          'Unit of analysis error ', 'A priori power calculations', 
                                                          'Conflict of interest ')

```

## Forest plot 

```{r - forest plot of univariate predicted results for risk of bias moderator variables}
bio_mods_plot_bias <- univariate_moderator_predictions_bias %>% 
  mutate(Level = factor(Level, levels = unique(Level))) %>% 
  group_by(Moderator)%>%
  ggplot(aes(x = Level, y= SMD, ymin=l95, ymax=u95, shape=shape))+
  scale_x_discrete(limits=rev) +
  scale_y_continuous(name="Standardised Mean Difference", limits=(c(-3,3)))+
  expand_limits(y=c(-3, 3))+
  geom_pointrange() +
  geom_hline(yintercept =0, linetype=2)+
  geom_errorbar(aes(ymin=l95, ymax=u95),width=0.05, cex=0)+
  theme_minimal()+
  coord_flip()+
  facet_wrap(~ Moderator, strip.position="left", labeller = label_wrap_gen(width=20), nrow=23, scales = "free_y")+
  geom_rect(aes(fill = Moderator),xmin = -Inf,xmax = Inf,
            ymin = -Inf,ymax = Inf,alpha = 0.1) +
  geom_point(shape=1, size=0.3)+
  labs(title = 'Estimated values for risk of bias moderator variables')+
  theme(plot.title=element_text(size=16,face="bold"),
        legend.position = "none",
        axis.text.y=element_text(),
        axis.ticks.y=element_blank(),
        axis.text.x=element_text(face="bold", ),
        axis.title.x =element_text(size=10,face="bold"),
        axis.title.y = element_blank(),
        strip.text.y.left = element_text(hjust=0,vjust = 0.5,angle=0,face="bold", size=10),
        strip.placement = "outside")

bio_mods_plot_bias
```

# Post hoc power calculations for univariate predictions 

## Biological/methodological variables

### 1. Species

```{r - estimate power for analysis of species as a moderator post hoc, results='hide'}
species_power <- mod_power(n_groups = 2,
                effect_sizes = c(-0.4689012, -0.5563216),
                study_size = 14,
                k = 48,
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(species_power)
```
6% power to detect to detect significant differences between species subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower)

### 2. Strain

```{r - estimate power for analysis of strain as a moderator post hoc, results='hide'}
strain_power <- mod_power(n_groups = 9,
                effect_sizes = c(-0.3502929, -0.3582353, -0.7065362, -0.5521387, -0.5521387, -0.005079909, -0.3775776, -1.009102,
                                 0.2092652),
                study_size = 14,
                k = 45,
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(strain_power)
```
29.6% power to detect to detect significant differences between species subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower)


### 4. Administration route

```{r - estimate power for analysis of administration route as a moderator post hoc, results='hide'}
admin_power <- mod_power(n_groups = 3,
                effect_sizes = c(-0.5619702, -0.4080437, 0.2092681),
                study_size = 14,
                k = 48,
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(admin_power)
```
51.9% power to detect to detect significant differences between administration route subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower)

### 5. Gestational stage at polyI:C injection 

```{r - estimate power for analysis of gestational stage as a moderator post hoc, results='hide'}
geststage_power <- mod_power(n_groups = 3,
                effect_sizes = c(-0.761603, -0.3490741, -0.4646616),
                study_size = 14,
                k = 48, #should be 47 but mod_power requires that n_groups is a factor of k
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(geststage_power)
```
16.7% power to detect to detect significant differences between sex subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower)

### 6. Developmental stage at PPI testing

```{r - estimate power for analysis of developmental stage as a moderator post hoc, results='hide'}
devstage_power <- mod_power(n_groups = 3,
                effect_sizes = c(-0.3293277, -0.06956562, -0.5764726),
                study_size = 14,
                k = 48, #should be 47 but mod_power requires that n_groups is a factor of k
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(devstage_power)
```
22.4% power to detect to detect significant differences between sex subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower)

## Risk of bias moderator variables 

### 1. Sequence generation 

```{r - estimate power for analysis of sequence generation RoB as a moderator post hoc, results='hide'}
seqgen_power <- mod_power(n_groups = 2,
                effect_sizes = c(-0.3354432, -0.7294316),
                study_size = 14,
                k = 48,
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(seqgen_power)
```
26.4% power to detect to detect significant differences between sequence generation subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower)

### 2. Allocation concealment

```{r - estimate power for analysis of allocation concealment RoB as a moderator post hoc, results='hide'}
allocconc_power <- mod_power(n_groups = 3,
                effect_sizes = c(-0.5295303, -0.03848536, -0.3246557),
                study_size = 14,
                k = 48,
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(allocconc_power)
```
21.5% power to detect to detect significant differences between allocation concealment subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower)

### 3. Blinding of animal handlers/carers

```{r - estimate power for analysis of blind handling RoB as a moderator post hoc, results='hide'}
blindhand_power <- mod_power(n_groups = 3,
                effect_sizes = c(-0.5295303, -0.03848536, -0.3246557),
                study_size = 14,
                k = 48,
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(blindhand_power)
```
21.5% power to detect to detect significant differences between blinding of animal handlers subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower). This is the same as for allocation concealment because all studies that didn't report allocation concealment also didn't report blinding of animal handlers/carers etc. 

### 4. Reporting of baseline characteristics 

```{r - estimate power for analysis of baseline characteristics RoB as a moderator post hoc, results='hide'}
basechar_power <- mod_power(n_groups = 2,
                effect_sizes = c(-0.4763733, -0.6908712),
                study_size = 14,
                k = 48,
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(basechar_power)
```
11.1% power to detect to detect significant differences between allocation concealment subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower)

### 4. Random housing of animals 

```{r - estimate power for analysis of random housing RoB as a moderator post hoc, results='hide'}
randhous_power <- mod_power(n_groups = 3,
                effect_sizes = c(-0.5059486, -0.8683457, 0.4452512),
                study_size = 14,
                k = 48,
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(randhous_power)
```
93.3% power to detect to detect significant differences between allocation concealment subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower)

### 5. Random outcome assessment

```{r - estimate power for analysis of random outcome RoB as a moderator post hoc, results='hide'}
randout_power <- mod_power(n_groups = 2,
                effect_sizes = c(-0.3812935, -0.8143362),
                study_size = 14,
                k = 48,
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(randout_power)
```
30.6% power to detect to detect significant differences between allocation concealment subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower)

### 6. Blind outcome assessment

```{r - estimate power for analysis of blind outcome RoB as a moderator post hoc, results='hide'}
blindout_power <- mod_power(n_groups = 3,
                effect_sizes = c(-0.5342228, -0.03858569, -0.4318953),
                study_size = 14,
                k = 48,
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(blindout_power)
```
23.7%% power to detect to detect significant differences between allocation concealment subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower)

### 7. Incomplete outcome data 

```{r - estimate power for analysis of incomplete data RoB as a moderator post hoc, results='hide'}
incompdata_power <- mod_power(n_groups = 3,
                effect_sizes = c(-0.3912025, -0.4365402, -0.6258117),
                study_size = 14,
                k = 48,
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(incompdata_power)
```
8.72% power to detect to detect significant differences between allocation concealment subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower)

### 8. Unit of analysis error 

```{r - estimate power for analysis of incomplete data RoB as a moderator post hoc, results='hide'}
uoaerror_power <- mod_power(n_groups = 3,
                effect_sizes = c(-1.27197, -0.4036561, -0.6223287),
                study_size = 14,
                k = 48,
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(uoaerror_power)
```
58.1% power to detect to detect significant differences between allocation concealment subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower)

### 8. A priori power calculations

```{r - estimate power for analysis of appower RoB as a moderator post hoc, results='hide'}
appower_power <- mod_power(n_groups = 2,
                effect_sizes = c(-0.484755, -1.454774),
                study_size = 14,
                k = 48,
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(appower_power)
```
88.2% power to detect to detect significant differences between allocation concealment subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower)

### 8. Conflict of interest power calculations

```{r - estimate power for analysis of coi RoB as a moderator post hoc, results='hide'}
coi_power <- mod_power(n_groups = 3,
                effect_sizes = c(-0.3744335, -1.135689, -0.4957748),
                study_size = 14,
                k = 48,
                i2 = .72,
                es_type = "d") #doesnt allow you calculate for hedges g

summary(coi_power)
```
50.1% power to detect to detect significant differences between allocation concealment subgroups (but this assumes even distribution of effect sizes to levels of the subgroup, so the actual power is much lower)



